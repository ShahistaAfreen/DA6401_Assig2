{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahistaAfreen/DA6401_Assig2/blob/main/NA21B050_DLA2_PartB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTVyOR9Q070J"
      },
      "source": [
        "**Q - 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWq1mINUqJ3N"
      },
      "source": [
        "##  Handling Image Dimension and Class Count Mismatches in Pre-trained Models\n",
        "\n",
        "###  Image Dimension Mismatch\n",
        "\n",
        "To address the potential difference in image dimensions between your naturalist dataset and ImageNet (on which many models are pre-trained):\n",
        "\n",
        "####  Solution:\n",
        "Use image resizing and rescaling transformations to match the input size expected by the pre-trained model.\n",
        "\n",
        "Most ImageNet models expect inputs of **224×224 pixels**.\n",
        "\n",
        "####  PyTorch Transformations:\n",
        "\n",
        "```python\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),                     # Resize shorter side to 256\n",
        "    transforms.CenterCrop(224),                # Crop to 224x224\n",
        "    transforms.ToTensor(),                     \n",
        "    transforms.Normalize(                       # Normalize using ImageNet's mean and std\n",
        "        mean=[0.485, 0.456, 0.406],             \n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZN7yTt1q6Wr"
      },
      "source": [
        "###  Class Count Mismatch\n",
        "\n",
        "Pre-trained ImageNet models have **1000 output classes**, but your naturalist dataset has **10 classes**.\n",
        "\n",
        "####  Solution:\n",
        "\n",
        "Replace the final classification layer of the pre-trained model with a new one that outputs **10 classes**.\n",
        "\n",
        "####  PyTorch Implementation:\n",
        "\n",
        "For models like **ResNet**:\n",
        "```python\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Optionally freeze earlier layers if you're only training the classifier\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQJER4jBIioG"
      },
      "source": [
        "**Q - 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iq6HVorrRJf"
      },
      "source": [
        "###  Question 2 – Strategies for Fine-Tuning Large Pretrained Models\n",
        "\n",
        "To keep the training computationally feasible while using large pre-trained models like ResNet50, VGG, EfficientNetV2, etc., the following fine-tuning strategies were used:\n",
        "\n",
        "-  **Freezing all layers except the final fully connected (classification) layer**  \n",
        "  - This significantly reduces the number of trainable parameters.\n",
        "  - The model only learns to classify based on pre-extracted features.\n",
        "\n",
        "-  **Freezing up to the last `k` layers and fine-tuning the remaining layers**  \n",
        "  - Example: Freeze all layers up to `layer3` in ResNet and fine-tune `layer4` and `fc`.\n",
        "  - Allows the model to adapt higher-level features to the new dataset.\n",
        "\n",
        "-  **Gradual unfreezing**  \n",
        "  - Initially freeze all layers, then progressively unfreeze and fine-tune deeper layers over epochs.\n",
        "  - Helps avoid catastrophic forgetting and stabilizes training.\n",
        "\n",
        "These strategies balance computational efficiency and model adaptability when transferring from ImageNet to the iNaturalist dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "\n",
        "import wandb\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "Z8eG6Vq1HLsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set global seed for reproducibility\n",
        "pl.seed_everything(42)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Updated dataset path to match your specific path\n",
        "dataset_path = '/content/drive/MyDrive/DL_A2_Dataset'\n",
        "train_dir = os.path.join(dataset_path, 'train')\n",
        "val_dir = os.path.join(dataset_path, 'validation')\n",
        "test_dir = os.path.join(dataset_path, 'val')  # For testing"
      ],
      "metadata": {
        "id": "dAPIASZnHRE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6CKi-aJnvO8e"
      },
      "outputs": [],
      "source": [
        "# Utility to generate a balanced dataset sample\n",
        "def create_balanced_subset(dataset, max_samples_per_class=100):\n",
        "    import random\n",
        "    classwise_indices = {i: [] for i in range(len(dataset.classes))}\n",
        "    for idx, (_, label) in enumerate(dataset.samples):\n",
        "        classwise_indices[label].append(idx)\n",
        "\n",
        "    selected_indices = []\n",
        "    for label_indices in classwise_indices.values():\n",
        "        count = min(max_samples_per_class, len(label_indices))\n",
        "        selected_indices.extend(random.sample(label_indices, count))\n",
        "\n",
        "    return Subset(dataset, selected_indices)\n",
        "\n",
        "\n",
        "# LightningModule using ResNet50 with various finetuning options\n",
        "class FinetuneResNet(pl.LightningModule):\n",
        "    def __init__(self, mode=\"head_only\", unfreeze_from=5):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.learning_rate = 1e-4\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        base_model = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
        "        orig_fc = base_model.fc\n",
        "\n",
        "        base_model.fc = nn.Sequential(\n",
        "            orig_fc,\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10)\n",
        "        )\n",
        "\n",
        "        self.model = base_model\n",
        "        self._set_trainable_layers(unfreeze_from)\n",
        "\n",
        "    def _set_trainable_layers(self, from_layer):\n",
        "        if self.mode == \"head_only\":\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.model.fc.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        elif self.mode == \"partial\":\n",
        "            for idx, child in enumerate(self.model.children()):\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = (idx >= from_layer)\n",
        "\n",
        "        elif self.mode == \"last_block\":\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.requires_grad = \"layer4\" in name or \"fc\" in name\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log_dict({\"train_loss\": loss, \"train_accuracy\": acc}, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        val_loss = self.loss_fn(logits, y)\n",
        "        val_acc = (logits.argmax(dim=1) == y).float().mean()\n",
        "        self.log_dict({\"val_loss\": val_loss, \"val_accuracy\": val_acc}, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        trainable = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        return torch.optim.Adam(trainable, lr=self.learning_rate)\n",
        "\n",
        "\n",
        "# Custom DataModule for managing train/val/test splits\n",
        "class ResNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.train_dir = train_dir\n",
        "        self.val_dir = val_dir\n",
        "        self.test_dir = test_dir\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.common_transform = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.4712, 0.4600, 0.3896], std=[0.2406, 0.2301, 0.2406])\n",
        "        ])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in [None, 'fit']:\n",
        "            self.train_data = ImageFolder(self.train_dir, transform=self.common_transform)\n",
        "            self.val_data = ImageFolder(self.val_dir, transform=self.common_transform)\n",
        "        if stage in [None, 'test']:\n",
        "            self.test_data = ImageFolder(self.test_dir, transform=self.common_transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Single experiment run with one strategy\n",
        "def run_experiment(strategy):\n",
        "    logger = WandbLogger(project=\"dl-resnet-experiments\", name=strategy)\n",
        "    model = FinetuneResNet(mode=strategy)\n",
        "    datamodule = ResNetDataModule()\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=5,\n",
        "        logger=logger,\n",
        "        accelerator=\"auto\",\n",
        "        callbacks=[EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=2)]\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, datamodule=datamodule)\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "# Execute for all selected strategies\n",
        "if __name__ == \"__main__\":\n",
        "    strategies = [\"head_only\", \"partial\", \"last_block\"]\n",
        "    for strategy in strategies:\n",
        "        run_experiment(strategy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_OdeI_wvbCT"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XO-5rNxkDFP"
      },
      "source": [
        "**Q - 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RQlkOeV1v4x"
      },
      "outputs": [],
      "source": [
        "# Verify dataset directories exist\n",
        "print(f\"Training path found: {os.path.exists(train_dir)}\")\n",
        "print(f\"Validation path found: {os.path.exists(val_dir)}\")\n",
        "print(f\"Testing path found: {os.path.exists(test_dir)}\")\n",
        "\n",
        "# LightningModule for transfer learning\n",
        "class ResNetFineTuner(pl.LightningModule):\n",
        "    def __init__(self, finetune_mode=\"layer4_fc\"):\n",
        "        super().__init__()\n",
        "        self.learning_rate = 1e-4\n",
        "        self.finetune_mode = finetune_mode\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Load pretrained ResNet50 model\n",
        "        model = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
        "        original_output = model.fc\n",
        "\n",
        "        # Modify classifier head\n",
        "        model.fc = nn.Sequential(\n",
        "            original_output,\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10)\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        self._set_trainable_layers()\n",
        "\n",
        "    def _set_trainable_layers(self):\n",
        "        \"\"\"Configure which layers to fine-tune.\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            param.requires_grad = (\"layer4\" in name or \"fc\" in name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = (outputs.argmax(1) == labels).float().mean()\n",
        "        self.log_dict({\"train_loss\": loss, \"train_accuracy\": acc}, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = (outputs.argmax(1) == labels).float().mean()\n",
        "        self.log_dict({\"val_loss\": loss, \"val_accuracy\": acc}, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, _):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = (outputs.argmax(1) == labels).float().mean()\n",
        "        self.log_dict({\"test_loss\": loss, \"test_accuracy\": acc}, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        params = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        return torch.optim.Adam(params, lr=self.learning_rate)\n",
        "\n",
        "# LightningDataModule for loading datasets\n",
        "class ImageClassificationData(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.train_path = train_dir\n",
        "        self.val_path = val_dir\n",
        "        self.test_path = test_dir\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        transform = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.4712, 0.4600, 0.3896], std=[0.2406, 0.2301, 0.2406])\n",
        "        ])\n",
        "\n",
        "        if stage in (\"fit\", None):\n",
        "            self.train_data = ImageFolder(self.train_path, transform=transform)\n",
        "            self.val_data = ImageFolder(self.val_path, transform=transform)\n",
        "\n",
        "        if stage in (\"test\", None):\n",
        "            self.test_data = ImageFolder(self.test_path, transform=transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Run the experiment\n",
        "def run_experiment():\n",
        "    model_name = \"resnet_last_block\"\n",
        "    wandb_logger = WandbLogger(project=\"image-classification\", name=model_name)\n",
        "\n",
        "    model = ResNetFineTuner(finetune_mode=\"layer4_fc\")\n",
        "    datamodule = ImageClassificationData()\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=5,\n",
        "        accelerator=\"auto\",\n",
        "        logger=wandb_logger\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, datamodule=datamodule)\n",
        "    trainer.test(model, datamodule=datamodule)\n",
        "    wandb.finish()\n",
        "\n",
        "# Trigger run\n",
        "run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XoU7O5vWtMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}